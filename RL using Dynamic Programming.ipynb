{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    def __init__(self, n, terminal_state, inplace = False, policy_iteration = True, verbose=False):\n",
    "        \n",
    "        self.grid_size = n\n",
    "        self.terminal_state = terminal_state\n",
    "        self.inplace = inplace\n",
    "        self.policy_iteration = policy_iteration\n",
    "        self.verbose = verbose\n",
    "        self.gamma = 0.1\n",
    "        \n",
    "        self.vt = np.zeros((self.grid_size, self.grid_size))\n",
    "        if not self.inplace:\n",
    "            self.vt1 = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        self.actions = [(0,-1), (0, 1), (-1,0), (1, 0)]# Left, Right, Up, Down\n",
    "\n",
    "        #Equi-probable action selection at each state for all actions\n",
    "        self.policy = np.random.randint(1,4,size = (self.grid_size, self.grid_size))\n",
    "        \n",
    "        print(\"Initial V\", *self.vt, sep='\\n',end='\\n\\n')\n",
    "        print(\"Initial Policy\", *self.policy, sep='\\n',end='\\n\\n')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Shape of V\", self.vt.shape)\n",
    "            print(\"Shape of Policy\", self.policy.shape)\n",
    "        \n",
    "    def _get_reward(self, x, y):\n",
    "        if x==self.terminal_state[0] and y==self.terminal_state[1]:\n",
    "            return 3\n",
    "        return -1\n",
    "    \n",
    "    def _get_next_state(self, x, y, a):\n",
    "        \n",
    "        if x+a[0]!=-1 and x+a[0]!=self.grid_size:\n",
    "            x = x + a[0]\n",
    "        if y+a[1]!=-1 and y+a[1]!=self.grid_size:\n",
    "            y = y + a[1]\n",
    "            \n",
    "        return x,y\n",
    "        \n",
    "    def play(self, epochs = 10, threshold = 0.1):\n",
    "        \n",
    "        if self.policy_iteration:\n",
    "            res = self._play_policy_iteration(epochs, threshold)\n",
    "            print(\"Final Policy:\")\n",
    "            print(*self.policy, sep = '\\n', end = '\\n\\n')\n",
    "            print(\"Final Value Function:\")\n",
    "            print(*self.vt, sep = '\\n', end = '\\n\\n')\n",
    "            return res\n",
    "        \n",
    "        else:\n",
    "            res = self._play_value_iteration(epochs, threshold)\n",
    "            print(\"Final Policy:\")\n",
    "            print(*self.policy, sep = '\\n', end = '\\n\\n')\n",
    "            print(\"Final Value Function:\")\n",
    "            print(*self.vt, sep = '\\n', end = '\\n\\n')\n",
    "            return res\n",
    "        \n",
    "    def _play_policy_iteration(self, epochs, threshold):\n",
    "        \n",
    "        \n",
    "        iters = 0\n",
    "        while True:\n",
    "            \n",
    "            iters+=1\n",
    "            \n",
    "            #Policy Evaluation:\n",
    "#             for _ in range(iters):\n",
    "            diff = 0\n",
    "            for i in range(self.grid_size):\n",
    "                for j in range(self.grid_size):\n",
    "\n",
    "                    old_V = self.vt[i, j]\n",
    "                    action = self.actions[self.policy[i,j]]\n",
    "                    new_x, new_y = self._get_next_state(i,j,action)\n",
    "                    reward = self._get_reward(new_x, new_y)\n",
    "\n",
    "                    if self.inplace:\n",
    "                        self.vt[i,j] = 0.25*(reward + self.gamma * self.vt[new_x, new_y])\n",
    "                        diff+= abs(old_V-self.vt[i,j])\n",
    "                    else:\n",
    "                        self.vt1[i,j] = 0.25*(reward + self.gamma * self.vt[new_x, new_y])\n",
    "                        diff+= abs(old_V-self.vt1[i,j])\n",
    "\n",
    "            if not self.inplace:\n",
    "                self.vt = np.array(self.vt1)\n",
    "                \n",
    "#                 if diff <= threshold:\n",
    "#                     break\n",
    "                    \n",
    "            if self.verbose and not self.inplace:\n",
    "                print(\"Ids of Vt and Vt+1\", id(self.vt), id(self.vt1))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Value function at iter:\", iters)\n",
    "                print(*self.vt, sep = '\\n', end = '\\n\\n')\n",
    "            \n",
    "            #Policy Improvement:\n",
    "            for i in range(self.grid_size):\n",
    "                for j in range(self.grid_size):\n",
    "                    \n",
    "                    best_action = -1\n",
    "                    best_value = -10\n",
    "                    for k, a in enumerate(self.actions):\n",
    "                        \n",
    "                        new_x, new_y = self._get_next_state(i,j,a)\n",
    "                        reward = self._get_reward(new_x, new_y)\n",
    "                        if best_value< (0.25*(reward + self.gamma*self.vt[new_x, new_y])):\n",
    "                            best_value = (0.25*(reward + self.gamma*self.vt[new_x, new_y]))\n",
    "                            best_action = k\n",
    "                    \n",
    "                    self.policy[i,j] = best_action\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Policy at iter:\", iters)\n",
    "                print(*self.policy, sep = '\\n', end = '\\n\\n')\n",
    "            \n",
    "            if epochs==iters or diff<=threshold:\n",
    "                return diff, iters\n",
    "\n",
    "    def _play_value_iteration(self, epochs, threshold):\n",
    "        \n",
    "        \n",
    "        iters = 0\n",
    "        while True:\n",
    "            \n",
    "            diff = 0\n",
    "            for i in range(self.grid_size):\n",
    "                for j in range(self.grid_size):\n",
    "                    \n",
    "                    old_V = self.vt[i, j]\n",
    "                    best_action = -1\n",
    "                    best_value = -10\n",
    "                    for k, a in enumerate(self.actions):\n",
    "                        \n",
    "                        new_x, new_y = self._get_next_state(i,j,a)\n",
    "                        reward = self._get_reward(new_x, new_y)\n",
    "                        if best_value< (0.25*(reward + self.gamma*self.vt[new_x, new_y])):\n",
    "                            best_value = (0.25*(reward + self.gamma*self.vt[new_x, new_y]))\n",
    "                            best_action = k\n",
    "                            \n",
    "                    if self.inplace:\n",
    "                        self.vt[i,j] = best_value\n",
    "                        diff+= abs(old_V-self.vt[i,j])\n",
    "                    else:\n",
    "                        self.vt1[i,j] = best_value\n",
    "                        diff+= abs(old_V-self.vt1[i,j])\n",
    "                        \n",
    "                    self.policy[i,j] = best_action\n",
    "            \n",
    "            iters+=1\n",
    "            \n",
    "            if not self.inplace:\n",
    "                self.vt = np.array(self.vt1)\n",
    "            \n",
    "            if self.verbose and not self.inplace:    \n",
    "                print(\"Ids of Vt and Vt+1\", id(self.vt), id(self.vt1))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Value function at iter:\", iters)\n",
    "                print(*self.vt, sep = '\\n', end = '\\n\\n')\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Policy at iter:\", iters)\n",
    "                print(*self.policy, sep = '\\n', end = '\\n\\n')\n",
    "            \n",
    "            if epochs==iters or diff<=threshold:\n",
    "                return diff, iters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial V\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "Initial Policy\n",
      "[2 3 1 3]\n",
      "[2 2 2 3]\n",
      "[2 3 3 2]\n",
      "[1 2 1 1]\n",
      "\n",
      "Final Policy:\n",
      "[1 3 0 0]\n",
      "[1 3 0 0]\n",
      "[1 0 0 0]\n",
      "[1 2 0 0]\n",
      "\n",
      "Final Value Function:\n",
      "[-0.25578487 -0.23139462 -0.25578487 -0.25639462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "[ 0.74421513 -0.23139462  0.74421513 -0.23139462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "\n",
      "(1.1102230246251565e-16, 8)\n"
     ]
    }
   ],
   "source": [
    "#Inplace\n",
    "game = GridWorld(4, (2,1), inplace=True, policy_iteration = True)\n",
    "print(game.play(epochs=50, threshold=0.00000000000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial V\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "Initial Policy\n",
      "[3 2 1 1]\n",
      "[1 2 2 2]\n",
      "[3 3 3 3]\n",
      "[3 3 1 1]\n",
      "\n",
      "Final Policy:\n",
      "[1 3 0 0]\n",
      "[1 3 0 0]\n",
      "[1 0 0 0]\n",
      "[1 2 0 0]\n",
      "\n",
      "Final Value Function:\n",
      "[-0.25578487 -0.23139462 -0.25578487 -0.25639462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "[ 0.74421513 -0.23139462  0.74421513 -0.23139462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "\n",
      "(8.881784197001252e-16, 12)\n"
     ]
    }
   ],
   "source": [
    "game = GridWorld(4, (2,1), inplace=False, policy_iteration = True)\n",
    "print(game.play(epochs=50, threshold=0.00000000000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial V\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "Initial Policy\n",
      "[1 3 2 2]\n",
      "[3 2 1 2]\n",
      "[2 1 3 3]\n",
      "[1 2 3 2]\n",
      "\n",
      "Final Policy:\n",
      "[1 3 0 0]\n",
      "[1 3 0 0]\n",
      "[1 0 0 0]\n",
      "[1 2 0 0]\n",
      "\n",
      "Final Value Function:\n",
      "[-0.25578487 -0.23139462 -0.25578487 -0.25639462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "[ 0.74421513 -0.23139462  0.74421513 -0.23139462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "\n",
      "(8.881784197001252e-16, 7)\n"
     ]
    }
   ],
   "source": [
    "#Inplace\n",
    "game = GridWorld(4, (2,1), inplace=True, policy_iteration = False)\n",
    "print(game.play(epochs=50, threshold=0.00000000000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial V\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "Initial Policy\n",
      "[2 2 2 3]\n",
      "[1 2 3 2]\n",
      "[3 1 2 2]\n",
      "[3 2 2 3]\n",
      "\n",
      "Final Policy:\n",
      "[1 3 0 0]\n",
      "[1 3 0 0]\n",
      "[1 0 0 0]\n",
      "[1 2 0 0]\n",
      "\n",
      "Final Value Function:\n",
      "[-0.25578487 -0.23139462 -0.25578487 -0.25639462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "[ 0.74421513 -0.23139462  0.74421513 -0.23139462]\n",
      "[-0.23139462  0.74421513 -0.23139462 -0.25578487]\n",
      "\n",
      "(6.38378239159465e-16, 11)\n"
     ]
    }
   ],
   "source": [
    "game = GridWorld(4, (2,1), inplace=False, policy_iteration = False)\n",
    "print(game.play(epochs=50, threshold=0.00000000000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results clearly shows that inplace updation along with the Value Iteration converges fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
